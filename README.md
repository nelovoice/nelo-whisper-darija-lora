# nelo-whisper-darija-lora
This project explores the fine-tuning of the openai/whisper-small model on Moroccan Darija datasets, using both Latin-script and Arabic-script transcriptions. It leverages LoRA (Low-Rank Adaptation) with the PEFT library to reduce the number of trainable parameters and accelerate training.
